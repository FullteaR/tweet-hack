{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sentence_transformers-0.2.5-py3.8.egg/sentence_transformers/SentenceTransformer.py:52: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/usr/local/lib/python3.8/site-packages/sentence_transformers-0.2.5-py3.8.egg/sentence_transformers/SentenceTransformer.py:52: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46c273a3696b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import MySQLdb\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = json.load(open(\"/credential.json\", \"r\"))\n",
    "\n",
    "consumer_key = infos[\"consumer_key\"]\n",
    "consumer_secret = infos[\"consumer_secret\"]\n",
    "access_token = infos[\"access_token\"]\n",
    "access_token_secret = infos[\"access_token_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = MySQLdb.connect(user=\"root\", passwd=\"root\",\n",
    "                               host=\"d2_db\", db=\"mysql\", use_unicode=True, charset=\"utf8mb4\")\n",
    "\n",
    "#cmd = 'echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "#path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
    "#                           shell=True).communicate()[0]).decode('utf-8')\n",
    "m = MeCab.Tagger(\"-Owakati\")#.format(path))\n",
    "model_path = \"/training_bert_japanese\"\n",
    "model = SentenceTransformer(model_path, show_progress_bar=False)\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_to = re.compile(\"@[0-9a-zA-Z_]+ \")\n",
    "url = re.compile(\"https?://[0-9a-zA-Z-._~?&=/]+\")\n",
    "\n",
    "def process(tweet):\n",
    "    if tweet[:3] == \"RT \":  # リツイート\n",
    "        return \"\"\n",
    "    tweet = reply_to.sub(\"\", tweet)\n",
    "    tweet = url.sub(\"\", tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM d2.tweet\", con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iblank = re.compile(\"^[\\s\\t]*\\n$\")\n",
    "\n",
    "# https://crimnut.hateblo.jp/entry/2018/08/25/202253からhan2zen,zen2hanをコピーさせていただきました\n",
    "ASCII_ZENKAKU_CHARS = (\n",
    "    u'ａ', u'ｂ', u'ｃ', u'ｄ', u'ｅ', u'ｆ', u'ｇ', u'ｈ', u'ｉ', u'ｊ', u'ｋ',\n",
    "    u'ｌ', u'ｍ', u'ｎ', u'ｏ', u'ｐ', u'ｑ', u'ｒ', u'ｓ', u'ｔ', u'ｕ', u'ｖ',\n",
    "    u'ｗ', u'ｘ', u'ｙ', u'ｚ',\n",
    "    u'Ａ', u'Ｂ', u'Ｃ', u'Ｄ', u'Ｅ', u'Ｆ', u'Ｇ', u'Ｈ', u'Ｉ', u'Ｊ', u'Ｋ',\n",
    "    u'Ｌ', u'Ｍ', u'Ｎ', u'Ｏ', u'Ｐ', u'Ｑ', u'Ｒ', u'Ｓ', u'Ｔ', u'Ｕ', u'Ｖ',\n",
    "    u'Ｗ', u'Ｘ', u'Ｙ', u'Ｚ',\n",
    "    u'！', u'”', u'＃', u'＄', u'％', u'＆', u'’', u'（', u'）', u'＊', u'＋',\n",
    "    u'，', u'－', u'．', u'／', u'：', u'；', u'＜', u'＝', u'＞', u'？', u'＠',\n",
    "    u'［', u'￥', u'］', u'＾', u'＿', u'‘', u'｛', u'｜', u'｝', u'～', u'　'\n",
    ")\n",
    "\n",
    "ASCII_HANKAKU_CHARS = (\n",
    "    u'a', u'b', u'c', u'd', u'e', u'f', u'g', u'h', u'i', u'j', u'k',\n",
    "    u'l', u'm', u'n', u'o', u'p', u'q', u'r', u's', u't', u'u', u'v',\n",
    "    u'w', u'x', u'y', u'z',\n",
    "    u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H', u'I', u'J', u'K',\n",
    "    u'L', u'M', u'N', u'O', u'P', u'Q', u'R', u'S', u'T', u'U', u'V',\n",
    "    u'W', u'X', u'Y', u'Z',\n",
    "    u'!', u'\"', u'#', u'$', u'%', u'&', u'\\'', u'(', u')', u'*', u'+',\n",
    "    u',', u'-', u'.', u'/', u':', u';', u'<', u'=', u'>', u'?', u'@',\n",
    "    u'[', u\"¥\", u']', u'^', u'_', u'`', u'{', u'|', u'}', u'~', u' '\n",
    ")\n",
    "\n",
    "KANA_ZENKAKU_CHARS = (\n",
    "    u'ア', u'イ', u'ウ', u'エ', u'オ', u'カ', u'キ', u'ク', u'ケ', u'コ',\n",
    "    u'サ', u'シ', u'ス', u'セ', u'ソ', u'タ', u'チ', u'ツ', u'テ', u'ト',\n",
    "    u'ナ', u'ニ', u'ヌ', u'ネ', u'ノ', u'ハ', u'ヒ', u'フ', u'ヘ', u'ホ',\n",
    "    u'マ', u'ミ', u'ム', u'メ', u'モ', u'ヤ', u'ユ', u'ヨ',\n",
    "    u'ラ', u'リ', u'ル', u'レ', u'ロ', u'ワ', u'ヲ', u'ン',\n",
    "    u'ァ', u'ィ', u'ゥ', u'ェ', u'ォ', u'ッ', u'ャ', u'ュ', u'ョ',\n",
    "    u'。', u'、', u'・', u'゛', u'゜', u'「', u'」', u'ー'\n",
    ")\n",
    "\n",
    "KANA_HANKAKU_CHARS = (\n",
    "    u'ｱ', u'ｲ', u'ｳ', u'ｴ', u'ｵ', u'ｶ', u'ｷ', u'ｸ', u'ｹ', u'ｺ',\n",
    "    u'ｻ', u'ｼ', u'ｽ', u'ｾ', u'ｿ', u'ﾀ', u'ﾁ', u'ﾂ', u'ﾃ', u'ﾄ',\n",
    "    u'ﾅ', u'ﾆ', u'ﾇ', u'ﾈ', u'ﾉ', u'ﾊ', u'ﾋ', u'ﾌ', u'ﾍ', u'ﾎ',\n",
    "    u'ﾏ', u'ﾐ', u'ﾑ', u'ﾒ', u'ﾓ', u'ﾔ', u'ﾕ', u'ﾖ',\n",
    "    u'ﾗ', u'ﾘ', u'ﾙ', u'ﾚ', u'ﾛ', u'ﾜ', u'ｦ', u'ﾝ',\n",
    "    u'ｧ', u'ｨ', u'ｩ', u'ｪ', u'ｫ', u'ｯ', u'ｬ', u'ｭ', u'ｮ',\n",
    "    u'｡', u'､', u'･', u'ﾞ', u'ﾟ', u'｢', u'｣', u'ｰ'\n",
    ")\n",
    "\n",
    "DIGIT_ZENKAKU_CHARS = (\n",
    "    u'０', u'１', u'２', u'３', u'４', u'５', u'６', u'７', u'８', u'９'\n",
    ")\n",
    "\n",
    "DIGIT_HANKAKU_CHARS = (\n",
    "    u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9'\n",
    ")\n",
    "\n",
    "KANA_TEN_MAP = (\n",
    "    (u'ガ', u'ｶ'), (u'ギ', u'ｷ'), (u'グ', u'ｸ'), (u'ゲ', u'ｹ'), (u'ゴ', u'ｺ'),\n",
    "    (u'ザ', u'ｻ'), (u'ジ', u'ｼ'), (u'ズ', u'ｽ'), (u'ゼ', u'ｾ'), (u'ゾ', u'ｿ'),\n",
    "    (u'ダ', u'ﾀ'), (u'ヂ', u'ﾁ'), (u'ヅ', u'ﾂ'), (u'デ', u'ﾃ'), (u'ド', u'ﾄ'),\n",
    "    (u'バ', u'ﾊ'), (u'ビ', u'ﾋ'), (u'ブ', u'ﾌ'), (u'ベ', u'ﾍ'), (u'ボ', u'ﾎ'),\n",
    "    (u'ヴ', u'ｳ')\n",
    ")\n",
    "\n",
    "KANA_MARU_MAP = (\n",
    "    (u'パ', u'ﾊ'), (u'ピ', u'ﾋ'), (u'プ', u'ﾌ'), (u'ペ', u'ﾍ'), (u'ポ', u'ﾎ')\n",
    ")\n",
    "\n",
    "\n",
    "ascii_zh_table = {}\n",
    "ascii_hz_table = {}\n",
    "kana_zh_table = {}\n",
    "kana_hz_table = {}\n",
    "digit_zh_table = {}\n",
    "digit_hz_table = {}\n",
    "\n",
    "for (az, ah) in zip(ASCII_ZENKAKU_CHARS, ASCII_HANKAKU_CHARS):\n",
    "    ascii_zh_table[az] = ah\n",
    "    ascii_hz_table[ah] = az\n",
    "\n",
    "for (kz, kh) in zip(KANA_ZENKAKU_CHARS, KANA_HANKAKU_CHARS):\n",
    "    kana_zh_table[kz] = kh\n",
    "    kana_hz_table[kh] = kz\n",
    "\n",
    "for (dz, dh) in zip(DIGIT_ZENKAKU_CHARS, DIGIT_HANKAKU_CHARS):\n",
    "    digit_zh_table[dz] = dh\n",
    "    digit_hz_table[dh] = dz\n",
    "\n",
    "kana_ten_zh_table = {}\n",
    "kana_ten_hz_table = {}\n",
    "kana_maru_zh_table = {}\n",
    "kana_maru_hz_table = {}\n",
    "for (ktz, kth) in KANA_TEN_MAP:\n",
    "    kana_ten_zh_table[ktz] = kth\n",
    "    kana_ten_hz_table[kth] = ktz\n",
    "\n",
    "for (kmz, kmh) in KANA_MARU_MAP:\n",
    "    kana_maru_zh_table[kmz] = kmh\n",
    "    kana_maru_hz_table[kmh] = kmz\n",
    "\n",
    "del ASCII_ZENKAKU_CHARS, ASCII_HANKAKU_CHARS, \\\n",
    "    KANA_ZENKAKU_CHARS, KANA_HANKAKU_CHARS,\\\n",
    "    DIGIT_ZENKAKU_CHARS, DIGIT_HANKAKU_CHARS,\\\n",
    "    KANA_TEN_MAP, KANA_MARU_MAP\n",
    "\n",
    "kakko_zh_table = {\n",
    "    u'｟': u'⦅', u'｠': u'⦆',\n",
    "    u'『': u'｢', u'』': u'｣',\n",
    "    u'〚': u'⟦', u'〛': u'⟧',\n",
    "    u'〔': u'❲', u'〕': u'❳',\n",
    "    u'〘': u'⟬', u'〙': u'⟭',\n",
    "    u'《': u'⟪', u'》': u'⟫',\n",
    "    u'【': u'(', u'】': u')',\n",
    "    u'〖': u'(', u'〗': u')'\n",
    "}\n",
    "kakko_hz_table = {}\n",
    "for k, v in kakko_zh_table.items():\n",
    "    kakko_hz_table[v] = k\n",
    "\n",
    "\n",
    "def zen2han(text=\"\", ascii_=True, digit=True, kana=True, kakko=True, ignore=()):\n",
    "    result = []\n",
    "    for c in text:\n",
    "        if c in ignore:\n",
    "            result.append(c)\n",
    "        elif ascii_ and (c in ascii_zh_table):\n",
    "            result.append(ascii_zh_table[c])\n",
    "        elif digit and (c in digit_zh_table):\n",
    "            result.append(digit_zh_table[c])\n",
    "        elif kana and (c in kana_zh_table):\n",
    "            result.append(kana_zh_table[c])\n",
    "        elif kana and (c in kana_ten_zh_table):\n",
    "            result.append(kana_ten_zh_table[c] + u'ﾞ')\n",
    "        elif kana and (c in kana_maru_zh_table):\n",
    "            result.append(kana_maru_zh_table[c] + u'ﾟ')\n",
    "        elif kakko and (c in kakko_zh_table):\n",
    "            result.append(kakko_zh_table[c])\n",
    "        else:\n",
    "            result.append(c)\n",
    "    return \"\".join(result)\n",
    "\n",
    "\n",
    "def han2zen(text, ascii_=True, digit=True, kana=True, kakko=True, ignore=()):\n",
    "    result = []\n",
    "    for i, c in enumerate(text):\n",
    "        if c == u'ﾞ' or c == u'ﾟ':\n",
    "            continue\n",
    "        elif c in ignore:\n",
    "            result.append(c)\n",
    "        elif ascii_ and (c in ascii_hz_table):\n",
    "            result.append(ascii_hz_table[c])\n",
    "        elif digit and (c in digit_hz_table):\n",
    "            result.append(digit_hz_table[c])\n",
    "        elif kana and (c in kana_ten_hz_table) and (text[i + 1] == u'ﾞ'):\n",
    "            result.append(kana_ten_hz_table[c])\n",
    "        elif kana and (c in kana_maru_hz_table) and (text[i + 1] == u'ﾟ'):\n",
    "            result.append(kana_maru_hz_table[c])\n",
    "        elif kana and (c in kana_hz_table):\n",
    "            result.append(kana_hz_table[c])\n",
    "        elif kakko and (c in kakko_hz_table):\n",
    "            result.append(kakko_hz_table[c])\n",
    "        else:\n",
    "            result.append(c)\n",
    "    return \"\".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_to = re.compile(\"@[0-9a-zA-Z_]+ \")\n",
    "url = re.compile(\"https?://[0-9a-zA-Z-._~?&=/]+\")\n",
    "\n",
    "def process(tweet):\n",
    "    if tweet[:2]==\"RT\":\n",
    "        return \"\"\n",
    "    try:\n",
    "        tweet = reply_to.sub(\"@\", tweet)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tweet = url.sub(\"\", tweet)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tweet = re.sub(\"[！!]+\", \"!\", tweet)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tweet = re.sub(\"[？?]+\", \"?\", tweet)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tweet = han2zen(tweet, ascii_=False, digit=False,\n",
    "                        kakko=False, kana=True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tweet = zen2han(tweet, kana=False)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tweet = re.sub(\"\\d+\", \"0\", tweet)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tweet = re.sub(\"\\n+\",\"\\n\",tweet)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tweet = re.sub(\"\\s+\",\" \",tweet)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        tweet = re.sub(\"[^ぁ-んァ-ヶー一-龠\\x00-\\x7F]+\",\"\",tweet)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].progress_apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_favs = pd.DataFrame(columns=[\"tweet_id\",\"created_at\",\"text\",\"user_id\",\"tweet_json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        for tweet in tweepy.Cursor(api.favorites).items():\n",
    "            tweet_id = tweet.id\n",
    "            created_at = tweet.created_at\n",
    "            print(created_at)\n",
    "            text = tweet.text\n",
    "            user_id = tweet.user.id\n",
    "            tweet_json = json.dumps(tweet._json, ensure_ascii=False)\n",
    "            s = pd.Series([tweet_id, created_at, text, user_id, tweet_json], index=df_favs.columns)\n",
    "            df_favs = df_favs.append(s, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"favs\"] = df[\"tweet_id\"].progress_apply(lambda w: w in df_favs.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectors = np.asarray(model.encode(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.XGBRegressor(n_estimators=100, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.fit(vectors, df[\"favs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bst, open(\"bst_fav.pickle\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
